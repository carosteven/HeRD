##############################################
# config for loading box-delivery simulation #
##############################################
### GLOBAL PARAMS
output_dir: "logs/" # 'output/trial0'

plot:
  y_axis_limit: 18  # limits the y-axis of the plot
  show: false       # true will show planner plots


render_scale: 60      # scale renderer window to fit screen size
# render_scale: 30

### SIMULATION PARAMS
render:
  log_obs: false        # whether to log occupancy observation
  show: true            # if true display the environment
  show_obs: false # if true show occupancy observation
sim:
  t_max: 2000  # max number of iterations in simulation loop (originally 5000)
  steps: 100    # number of simulation steps per iteration (originally 10)
  gravity: !!python/tuple
    - 0
    - 0
  iterations: 10  # controls accuracy of pymunk solver i.e. simulation accuracy, default is 10
  damping: 0      # damping to body velocities

### CONTROLLER PARAMS
controller:
  dt: 0.2  # the agent dynamics model was discretized with this dt (originally 0.02)
  # target speed
  target_speed: 0.3  # m/s

boxes:
  num_boxes_small: 10
  num_boxes_large: 20
  box_size: 0.44
  min_box_dist: 0.62       # minimum box distance during random initialization
  box_density: 0.001

agent:
  action_type: 'position' # options are velocity, heading, position
  step_size: 1.25 # distance travelled per step in heading control
  random_start: true
  mass: 1  # mass of the robot
  padding: 0.25  # adds padding to ship footprint
  length: 0.6
  width: 0.92
  footprint_vertices: [ # Used for the observation classes
              [0.30, -0.53],
              [-0.30, -0.39],

              [-0.30, 0.39],
              [0.30, 0.53],
  ]
  vertices: [[0.26, -0.35],
              [-0.31, -0.35],

              [-0.31, 0.35],
              [0.26, 0.35],
          ]
  front_bumper_vertices: [
              [0.31, -0.53],
              [0.20, -0.39],
              [0.20, 0.39],
              [0.31, 0.53],     
              ]
  wheel_vertices: [
    # left front
    [
        [0.09, 0.35],
        [0.09, 0.39],
        [0.24, 0.39], 
        [0.24, 0.35]
    ],

    # right front
    [
        [0.09, -0.39],
        [0.09, -0.35],
        [0.24, -0.35], 
        [0.24, -0.39]
    ],

    # left rear
    [
        [-0.09, 0.35],
        [-0.09, 0.39],
        [-0.24, 0.39], 
        [-0.24, 0.35]
    ],

    # right rear
    [
        [-0.09, -0.35],
        [-0.09, -0.39],
        [-0.24, -0.39], 
        [-0.24, -0.35]
    ],
  ]

### ENVIRONMENT PARAMS
env:
  obstacle_config: small_empty # options are small_empty, small_columns, large_columns, large_divider
  room_length: 10
  room_width_small: 5
  room_width_large: 10
  receptacle_width: 1.5
  shortest_path_channel_scale: 0.25
  local_map_pixel_width: 96
  local_map_width: 10 # 10 meters
  wall_thickness: 14
  invert_receptacle_map: false

misc:
  ministep_size: 2.5
  inactivity_cutoff: 100
  random_seed: 42

rewards:
  partial_rewards_scale: 0.2
  goal_reward: 1.0
  collision_penalty: 0.25
  non_movement_penalty: 0.25
  max_distance_reward: false
  step_dist_penalty: false

train:
  train_mode: false
  job_name: 'sam_model'
  batch_size: 32
  checkpoint_freq: 6000
  exploration_timesteps: 6000
  final_exploration: 0.01
  gamma: 0.99
  grad_norm_clipping: 10
  job_id_to_resume: null
  learning_rate: 0.01
  learning_starts: 1000
  n_epochs: 10
  n_steps: 256
  output_log_dir: 'output_logs/'
  random_env: False
  replay_buffer_size: 10000
  resume_training: false
  target_update_freq: 1000
  total_timesteps: 60000
  verbose: 2
  weight_decay: 0.0001

evaluate:
  eval_mode: true
  final_exploration: 0.0
  num_eps: 2
  model_path: 'models'
  policy_types: ['ppo', 'sac', 'sam']  # list of policy types to evaluate
  action_types: ['heading', 'heading', 'position']  # list of action types to evaluate
  models: ['ppo_small_empty', 'sac_small_empty', 'sam_small_empty']  # list of model names to evaluate
  obs_configs: ['small_empty', 'small_empty', 'small_empty']  # list of obstacle configurations to evaluate

rl_policy:
  model_path: models/rl_models
  model_name: training_test

diffusion:
  use_diffusion_policy: true
  horizon: 32
  action_dim: 2
  obs_dim: 26 # for combo
  obs_type: combo # positions or vertices
  n_action_steps: 32
  n_obs_steps: 1
  num_inference_steps: 15
  model_path: models/diffusion_models/
  model_name: herd_diffusion_model.ckpt

demonstration:
  demonstration_mode: false
  teleop_mode: false     # when using teleoperation mode, control both forward and angular speed
  step_size: 0.3        # distance interval to save demonstrations
